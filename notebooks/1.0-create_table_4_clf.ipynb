{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sys\n",
    "import json\n",
    "#import stanza\n",
    "#stanza.download('pt')\n",
    "import numpy as np\n",
    "# install and import amr utils\n",
    "sys.path.append('../')\n",
    "#!pip install ../amr-utils\n",
    "from amr_utils.amr_readers import AMR_Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '../POS-tagger-portuguese-nltk/trained_POS_taggers/'\n",
    "tagger_nltk = joblib.load(folder+'POS_tagger_brill.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feat_spacy(text,nlp_model):\n",
    "    \n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    \n",
    "    tokens = []\n",
    "\n",
    "    dict_an_snt = {}\n",
    "\n",
    "    for i,token in enumerate(doc):\n",
    "        \n",
    "        tokens.append(token.text)\n",
    "        \n",
    "        dict_an_snt.update({\n",
    "            \n",
    "            \n",
    "            i: {\"text\": token.text,\n",
    "            \"lemma\": token.lemma_,\n",
    "            \"pos\":token.pos_,\n",
    "            \"tag\":token.tag_,\n",
    "            \"dep\":token.dep_,\n",
    "            \"shape\":token.shape_,\n",
    "            \"is_alpha\":token.is_alpha,\n",
    "            \"is_stop\":token.is_stop,\n",
    "            \"morph\": str(token.morph),\n",
    "            \"head_index\": token.head.i,\n",
    "            \"ner\": None \n",
    "            }\n",
    "        })\n",
    "        \n",
    "    for ent in doc.ents:\n",
    "        \n",
    "        for i in range(ent.start,ent.end): \n",
    "            dict_an_snt[i][\"ner\"] = ent.label_\n",
    "            dict_an_snt[i][\"ner_start_end\"] = (ent.start,ent.end)\n",
    "            \n",
    "    response = {\n",
    "        \"sentence\": text,\n",
    "        \"tokens\": tokens,\n",
    "        \"annotated_sentence\": dict_an_snt\n",
    "    }\n",
    "            \n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amr_to_dict(amr):\n",
    "    \n",
    "    dict_amr = {}\n",
    "    id_snt = amr.id\n",
    "    nodes = amr.nodes\n",
    "    metadata = amr.metadata\n",
    "    tokens = amr.tokens\n",
    "    graph = amr.graph_string()\n",
    "    \n",
    "    dict_edges = {}\n",
    "    id = 0\n",
    "    # cria dict dos nos\n",
    "    for node1_id, edge_value, node2_id in amr.edges:\n",
    "        id +=1\n",
    "        \n",
    "        dict_edges.update({\n",
    "            f'edge {id}': {\n",
    "                'nodes_ids': (node1_id,node2_id),\n",
    "                'nodes': (nodes.get(node1_id),nodes.get(node2_id)),\n",
    "                'value': edge_value\n",
    "            }})\n",
    "        \n",
    "    if tokens == []:\n",
    "        snt = metadata['snt']\n",
    "        tokens_nltk = word_tokenize(snt, language='portuguese')\n",
    "    else: \n",
    "        snt = \" \".join(tokens)\n",
    "        tokens_nltk = word_tokenize(snt, language='portuguese')\n",
    "      \n",
    "    \n",
    "    # verfica se ha tokens na anotacao (se nao tiver tokens, ele considera snt como tokens)\n",
    "    if \"tok pt\" not in amr.amr_string():\n",
    "        tokens = []\n",
    "        \n",
    "    dict_amr.update({'id': id_snt})\n",
    "    dict_amr.update({'nodes': nodes})\n",
    "    dict_amr.update({\"edges\": dict_edges})\n",
    "    dict_amr.update(metadata)\n",
    "    dict_amr.update({\"graph\": graph})\n",
    "    dict_amr.update({\"tok pt\": tokens})\n",
    "    dict_amr.update({\"tokens_nltk\": tokens_nltk})\n",
    "    dict_amr.update({\"snt\": snt})\n",
    "    return dict_amr\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def ner_with_spacy(\n",
    "    text,\n",
    "    model_name = \"pt_core_news_sm\"\n",
    "):\n",
    "    nlp = spacy.load(model_name)\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    entidades = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        entidade = (ent.text,  ent.label_,(ent.start,ent.end))\n",
    "        entidades.append(entidade)\n",
    "        \n",
    "    return entidades\n",
    "\n",
    "def pos_tagger_nltk(\n",
    "    tokenized_sentence,\n",
    "    tagger_nltk\n",
    "):\n",
    "    \n",
    "    # anota os tokens \n",
    "    pos_tags_annotation = tagger_nltk.tag(tokenized_sentence)\n",
    "    \n",
    "    # cria lista apenas com os tags, sem o token\n",
    "    pos_tags = [tag[1] for tag in pos_tags_annotation]\n",
    "    \n",
    "    return pos_tags\n",
    "\n",
    "def remove_num_text(string):\n",
    "    \n",
    "    if type(string) != str:\n",
    "        return None \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Expressão regular para corresponder ao padrão [palavra]-[numero]\n",
    "    padrao = re.compile(r'^(\\w+)-\\d+$')\n",
    "    \n",
    "    # Tentativa de encontrar correspondência\n",
    "    correspondencia = padrao.match(string)\n",
    "    \n",
    "    if correspondencia:\n",
    "        # Se encontrar, retorna apenas a palavra (primeiro grupo da regex)\n",
    "        return correspondencia.group(1)\n",
    "    else:\n",
    "        # Caso contrário, retorna a string original\n",
    "        return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_lp = '../data/raw/little_prince.txt'\n",
    "path_opisums = '../data/raw/opisums.txt'\n",
    "path_news = '../data/raw/news.txt'\n",
    "path_sci = '../data/raw/science.txt'\n",
    "\n",
    "list_paths = [\n",
    "    ('sci',path_sci),\n",
    "    \n",
    "    ('lp', path_lp),\n",
    "    ('opisums',path_opisums),\n",
    "    ('news',path_news),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_alignment(text):\n",
    "    alignment_line = \"\"\n",
    "    \n",
    "    # Procura pela linha que contém os alinhamentos bramr\n",
    "    for line in text.split('\\n'):\n",
    "        if line.startswith(\"# ::alignments-bramr\"):\n",
    "            alignment_line = line\n",
    "            break\n",
    "    \n",
    "    # Verifica se a linha de alinhamento está vazia ou não foi encontrada\n",
    "    if not alignment_line or alignment_line.strip() == \"# ::alignments-bramr\":\n",
    "        return None\n",
    "    \n",
    "    # Remove o prefixo para obter os alinhamentos\n",
    "    alignment_line = alignment_line.replace(\"# ::alignments-bramr \", \"\").strip()\n",
    "    \n",
    "    # Inicializa o dicionário de resultados\n",
    "    alignment_dict = {}\n",
    "    \n",
    "    # Processa cada par de alinhamento\n",
    "    for pair in alignment_line.split():\n",
    "        token_index, node = pair.split('-')\n",
    "        alignment_dict[node] = int(token_index)\n",
    "    \n",
    "    return alignment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cannot deinvert attribute: ('s2', ':op2-of', 'and')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "# Running: sci\n",
      "##################################################\n",
      "[amr] Loading AMRs from file: ../data/raw/science.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:00<00:00, 2050.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anotando texto ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:00<00:00, 167.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################\n",
      "# Running: lp\n",
      "##################################################\n",
      "[amr] Loading AMRs from file: ../data/raw/little_prince.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1525/1525 [00:00<00:00, 4567.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anotando texto ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1525/1525 [00:08<00:00, 185.34it/s]\n",
      "ignoring epigraph data for duplicate triple: ('p', ':instance', 'pai')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################\n",
      "# Running: opisums\n",
      "##################################################\n",
      "[amr] Loading AMRs from file: ../data/raw/opisums.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404/404 [00:00<00:00, 4344.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anotando texto ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404/404 [00:02<00:00, 182.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################\n",
      "# Running: news\n",
      "##################################################\n",
      "[amr] Loading AMRs from file: ../data/raw/news.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 870/870 [00:00<00:00, 5918.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anotando texto ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 870/870 [00:04<00:00, 195.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_features = pd.DataFrame({\n",
    "    'sentence_id': [],\n",
    "    \"corpus_name\": [],\n",
    "    'parent': [],\n",
    "    'child': [],\n",
    "    'parent_pos': [],\n",
    "    'child_pos': [],\n",
    "    'parent_ner': [],\n",
    "    'child_ner': [],\n",
    "    'dependency_role': [],\n",
    "    'parent_position': [],\n",
    "    'child_position': [],\n",
    "    'label':[]\n",
    "})\n",
    "\n",
    "nlp_model_spacy = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "list_dict_an = []\n",
    "for corpus_name, path in list_paths:\n",
    "    \n",
    "    print(f\"\"\"##################################################\n",
    "# Running: {corpus_name}\n",
    "##################################################\"\"\")\n",
    "        \n",
    "    # usa reader de amr para fazer o parsing\n",
    "    reader = AMR_Reader()\n",
    "    amrs = reader.load(path, remove_wiki=True)\n",
    "    \n",
    "    # cria bloco de textos anotados\n",
    "    with open(path, 'r') as file:\n",
    "        str_corpus = file.read()\n",
    "    blocos = str_corpus.split('\\n\\n')\n",
    "    blocos = [bloco for bloco in blocos if bloco != \"\"]\n",
    "        \n",
    "    list_dicts = []\n",
    "    # para cada sentenca, cria dict com os nós e vértices\n",
    "    for i,amr in tqdm(enumerate(amrs), total = len(amrs)):\n",
    "        \n",
    "        texto_anotado = blocos[i]\n",
    "        dict_aligments = parse_alignment(texto_anotado)\n",
    "        \n",
    "        amr_string = amr.amr_string() # obtem string formatada amr        \n",
    "        \n",
    "        dict_annotation = amr_to_dict(amr) \n",
    "        dict_annotation.update({'corpus_name': corpus_name})\n",
    "        dict_annotation.update({'dict_aligments': dict_aligments})\n",
    "        list_dicts.append(dict_annotation)\n",
    "        \n",
    "    print('Anotando texto ...')    \n",
    "    for dict_an in tqdm(list_dicts):\n",
    "        \n",
    "        tokens = dict_an['tokens_nltk']\n",
    "        \n",
    "        if tokens == []:\n",
    "            tokens = dict_an['tok pt']\n",
    "        \n",
    "        # tok_pos = pos_tagger_nltk(\n",
    "        # tokenized_sentence = tokens,\n",
    "        # tagger_nltk = tagger_nltk\n",
    "        # )\n",
    "          \n",
    "        dict_spacy = extract_feat_spacy(\n",
    "            text = dict_an['snt'],\n",
    "            nlp_model = nlp_model_spacy)\n",
    "        \n",
    "                \n",
    "        dict_an.update(dict_spacy)\n",
    "        #dict_an.update({'tok pos': tok_pos})\n",
    "        dict_an.update({\"corpus_name\": corpus_name})\n",
    "        \n",
    "        list_dict_an.append(dict_an)\n",
    "\n",
    "    print()\n",
    "    \n",
    "with open('../data/processed/annotated_text.json', 'w') as f:\n",
    "    json.dump(list_dict_an, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos que não podem estar contidas no amr (descritas no artigo)\n",
    "# punct tambem removi pq tbm nao pode e esta no artigo de certa forma\n",
    "pos_filter = [\"SCONJ\", \"DET\", \"ADJ\", \"PUNCT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2959 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2959/2959 [00:00<00:00, 17209.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "True False\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "True False\n",
      "False False\n",
      "True True\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False True\n",
      "True True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "True False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "True True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "True False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "True False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "True False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "True False\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "True True\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "True False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "True False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "True True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "True False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "True False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "True False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "True True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "True False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "True False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "True True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "True True\n",
      "True False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "True False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "True False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "True False\n",
      "False True\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "True False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "True False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "True False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "True False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "True False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False True\n",
      "False False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus_name</th>\n",
       "      <th>id</th>\n",
       "      <th>parent</th>\n",
       "      <th>child</th>\n",
       "      <th>label</th>\n",
       "      <th>dep</th>\n",
       "      <th>parent_text</th>\n",
       "      <th>parent_lemma</th>\n",
       "      <th>parent_pos</th>\n",
       "      <th>parent_tag</th>\n",
       "      <th>...</th>\n",
       "      <th>child_lemma</th>\n",
       "      <th>child_pos</th>\n",
       "      <th>child_tag</th>\n",
       "      <th>child_shape</th>\n",
       "      <th>child_is_alpha</th>\n",
       "      <th>child_is_stop</th>\n",
       "      <th>child_morph</th>\n",
       "      <th>child_ner</th>\n",
       "      <th>child_ner_start_end</th>\n",
       "      <th>parent_ner_start_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sci</td>\n",
       "      <td>1</td>\n",
       "      <td>ter</td>\n",
       "      <td>person</td>\n",
       "      <td>:ARG0</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>tem</td>\n",
       "      <td>ter</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>...</td>\n",
       "      <td>Meyer</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Gender=Masc|Number=Sing</td>\n",
       "      <td>PER</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sci</td>\n",
       "      <td>1</td>\n",
       "      <td>person</td>\n",
       "      <td>explicar</td>\n",
       "      <td>:ARG1</td>\n",
       "      <td>nao_tem_dep</td>\n",
       "      <td>Meyer</td>\n",
       "      <td>Meyer</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>...</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sci</td>\n",
       "      <td>1</td>\n",
       "      <td>explicar</td>\n",
       "      <td>person</td>\n",
       "      <td>:ARG0</td>\n",
       "      <td>nao_tem_dep</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>...</td>\n",
       "      <td>Meyer</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Gender=Masc|Number=Sing</td>\n",
       "      <td>PER</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sci</td>\n",
       "      <td>1</td>\n",
       "      <td>explicar</td>\n",
       "      <td>coisa</td>\n",
       "      <td>:ARG1</td>\n",
       "      <td>nao_tem_dep</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>...</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sci</td>\n",
       "      <td>1</td>\n",
       "      <td>coisa</td>\n",
       "      <td>resultar</td>\n",
       "      <td>:ARG2-of</td>\n",
       "      <td>nao_tem_dep</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>...</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7041</th>\n",
       "      <td>news</td>\n",
       "      <td>esporte-4-169</td>\n",
       "      <td>person</td>\n",
       "      <td>confirmar</td>\n",
       "      <td>:ARG0-of</td>\n",
       "      <td>nao_tem_dep</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>...</td>\n",
       "      <td>confirmar</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbF...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7042</th>\n",
       "      <td>news</td>\n",
       "      <td>ciencia-4-171</td>\n",
       "      <td>possible</td>\n",
       "      <td>chamar</td>\n",
       "      <td>:ARG1</td>\n",
       "      <td>nao_tem_dep</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>...</td>\n",
       "      <td>chamar</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>VerbForm=Inf</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7043</th>\n",
       "      <td>news</td>\n",
       "      <td>ciencia-4-171</td>\n",
       "      <td>chamar</td>\n",
       "      <td>nós</td>\n",
       "      <td>:ARG0</td>\n",
       "      <td>nao_tem_dep</td>\n",
       "      <td>chamar</td>\n",
       "      <td>chamar</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>...</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7044</th>\n",
       "      <td>news</td>\n",
       "      <td>ciencia-4-171</td>\n",
       "      <td>chamar</td>\n",
       "      <td>fenômeno</td>\n",
       "      <td>:ARG1</td>\n",
       "      <td>obj</td>\n",
       "      <td>chamar</td>\n",
       "      <td>chamar</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>...</td>\n",
       "      <td>fenômeno</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Gender=Masc|Number=Sing</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7045</th>\n",
       "      <td>news</td>\n",
       "      <td>ciencia-4-171</td>\n",
       "      <td>chamar</td>\n",
       "      <td>efeito</td>\n",
       "      <td>:ARG2</td>\n",
       "      <td>nao_tem_dep</td>\n",
       "      <td>chamar</td>\n",
       "      <td>chamar</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>...</td>\n",
       "      <td>efeito</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Gender=Masc|Number=Sing</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7046 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     corpus_name             id    parent      child     label          dep  \\\n",
       "0            sci              1       ter     person     :ARG0        nsubj   \n",
       "1            sci              1    person   explicar     :ARG1  nao_tem_dep   \n",
       "2            sci              1  explicar     person     :ARG0  nao_tem_dep   \n",
       "3            sci              1  explicar      coisa     :ARG1  nao_tem_dep   \n",
       "4            sci              1     coisa   resultar  :ARG2-of  nao_tem_dep   \n",
       "...          ...            ...       ...        ...       ...          ...   \n",
       "7041        news  esporte-4-169    person  confirmar  :ARG0-of  nao_tem_dep   \n",
       "7042        news  ciencia-4-171  possible     chamar     :ARG1  nao_tem_dep   \n",
       "7043        news  ciencia-4-171    chamar        nós     :ARG0  nao_tem_dep   \n",
       "7044        news  ciencia-4-171    chamar   fenômeno     :ARG1          obj   \n",
       "7045        news  ciencia-4-171    chamar     efeito     :ARG2  nao_tem_dep   \n",
       "\n",
       "                        parent_text                   parent_lemma  \\\n",
       "0                               tem                            ter   \n",
       "1                             Meyer                          Meyer   \n",
       "2     token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "3     token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "4     token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "...                             ...                            ...   \n",
       "7041  token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "7042  token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "7043                         chamar                         chamar   \n",
       "7044                         chamar                         chamar   \n",
       "7045                         chamar                         chamar   \n",
       "\n",
       "                         parent_pos                     parent_tag  ...  \\\n",
       "0                              VERB                           VERB  ...   \n",
       "1                             PROPN                          PROPN  ...   \n",
       "2     token_nao_encontrado_no_texto  token_nao_encontrado_no_texto  ...   \n",
       "3     token_nao_encontrado_no_texto  token_nao_encontrado_no_texto  ...   \n",
       "4     token_nao_encontrado_no_texto  token_nao_encontrado_no_texto  ...   \n",
       "...                             ...                            ...  ...   \n",
       "7041  token_nao_encontrado_no_texto  token_nao_encontrado_no_texto  ...   \n",
       "7042  token_nao_encontrado_no_texto  token_nao_encontrado_no_texto  ...   \n",
       "7043                           VERB                           VERB  ...   \n",
       "7044                           VERB                           VERB  ...   \n",
       "7045                           VERB                           VERB  ...   \n",
       "\n",
       "                        child_lemma                      child_pos  \\\n",
       "0                             Meyer                          PROPN   \n",
       "1     token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "2                             Meyer                          PROPN   \n",
       "3     token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "4     token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "...                             ...                            ...   \n",
       "7041                      confirmar                           VERB   \n",
       "7042                         chamar                           VERB   \n",
       "7043  token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "7044                       fenômeno                           NOUN   \n",
       "7045                         efeito                           NOUN   \n",
       "\n",
       "                          child_tag                    child_shape  \\\n",
       "0                             PROPN                          Xxxxx   \n",
       "1     token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "2                             PROPN                          Xxxxx   \n",
       "3     token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "4     token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "...                             ...                            ...   \n",
       "7041                           VERB                           xxxx   \n",
       "7042                           VERB                           xxxx   \n",
       "7043  token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "7044                           NOUN                           xxxx   \n",
       "7045                           NOUN                           xxxx   \n",
       "\n",
       "                     child_is_alpha                  child_is_stop  \\\n",
       "0                              True                          False   \n",
       "1     token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "2                              True                          False   \n",
       "3     token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "4     token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "...                             ...                            ...   \n",
       "7041                           True                          False   \n",
       "7042                           True                          False   \n",
       "7043  token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "7044                           True                          False   \n",
       "7045                           True                          False   \n",
       "\n",
       "                                            child_morph  \\\n",
       "0                               Gender=Masc|Number=Sing   \n",
       "1                         token_nao_encontrado_no_texto   \n",
       "2                               Gender=Masc|Number=Sing   \n",
       "3                         token_nao_encontrado_no_texto   \n",
       "4                         token_nao_encontrado_no_texto   \n",
       "...                                                 ...   \n",
       "7041  Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbF...   \n",
       "7042                                       VerbForm=Inf   \n",
       "7043                      token_nao_encontrado_no_texto   \n",
       "7044                            Gender=Masc|Number=Sing   \n",
       "7045                            Gender=Masc|Number=Sing   \n",
       "\n",
       "                          child_ner child_ner_start_end parent_ner_start_end  \n",
       "0                               PER              (0, 1)                  NaN  \n",
       "1     token_nao_encontrado_no_texto                 NaN               (0, 1)  \n",
       "2                               PER              (0, 1)                  NaN  \n",
       "3     token_nao_encontrado_no_texto                 NaN                  NaN  \n",
       "4     token_nao_encontrado_no_texto                 NaN                  NaN  \n",
       "...                             ...                 ...                  ...  \n",
       "7041                           None                 NaN                  NaN  \n",
       "7042                           None                 NaN                  NaN  \n",
       "7043  token_nao_encontrado_no_texto                 NaN                  NaN  \n",
       "7044                           None                 NaN                  NaN  \n",
       "7045                           None                 NaN                  NaN  \n",
       "\n",
       "[7046 rows x 26 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_nodes_ids(\n",
    "    annotated_sentence,\n",
    "    edge_amr_info\n",
    "):\n",
    "    \n",
    "    parent, child = edge_amr_info['nodes']\n",
    "    \n",
    "    \n",
    "    parent = remove_num_text(parent)\n",
    "    child = remove_num_text(child) \n",
    "    \n",
    "    \n",
    "\n",
    "    if parent != None:\n",
    "        \n",
    "        # acha a posicao de todos os tokens no texto que sao iguais a parent\n",
    "        parent_w_app = [id for id, an in annotated_sentence.items() if (an[\"lemma\"].casefold() == parent.casefold() or an[\"text\"].casefold() == parent.casefold()) and an[\"pos\"] not in pos_filter]\n",
    "        \n",
    "        # todo nome pessoal vira person na anotação, então precisa achar qual nome tem de pessoa tem no texto\n",
    "        if len(parent_w_app) == 0 and parent == \"person\":\n",
    "            \n",
    "            parent_w_app = [id for id, an in annotated_sentence.items() if an[\"ner\"] == \"PER\"]\n",
    "            \n",
    "            if len(parent_w_app) >1:\n",
    "                \n",
    "                # nomes compostos vao ocupar dois tokens. Para verificar se é um nome composto parta olha o inicio e fim do ner\n",
    "                inicio_fim = annotated_sentence[parent_w_app[0]]['ner_start_end'] # basta pegar o inicio e fim do primeiro token\n",
    "                \n",
    "                # verifica se o inicio e fim bate com os ids encontrados\n",
    "                contains = True\n",
    "                for i in parent_w_app:\n",
    "                    if i not in range(inicio_fim[0], inicio_fim[1]):\n",
    "                        contains = False\n",
    "                \n",
    "\n",
    "                if contains:\n",
    "                    \n",
    "                    parent_w_app = [tuple(parent_w_app)]\n",
    "        \n",
    "    else:\n",
    "\n",
    "        parent_w_app = None\n",
    "\n",
    "    if child != None:\n",
    "        \n",
    "        # acha a posicao de todos os tokens no texto que sao iguais a child\n",
    "        child_w_app = [id for id, an in annotated_sentence.items() if (an[\"lemma\"].casefold() == child.casefold() or  an[\"text\"].casefold() == child.casefold()) and an[\"pos\"] not in pos_filter]\n",
    "\n",
    "        # todo nome pessoal vira person na anotação, então precisa achar qual nome tem de pessoa tem no texto\n",
    "        if len(child_w_app) == 0 and child == \"person\":\n",
    "            \n",
    "            child_w_app = [id for id, an in annotated_sentence.items() if an[\"ner\"] == \"PER\"]\n",
    "            \n",
    "            if len(child_w_app) >1:\n",
    "                \n",
    "                # nomes compostos vao ocupar dois tokens. Para verificar se é um nome composto parta olha o inicio e fim do ner\n",
    "                inicio_fim = annotated_sentence[child_w_app[0]]['ner_start_end'] # basta pegar o inicio e fim do primeiro token\n",
    "                \n",
    "                # verifica se o inicio e fim bate com os ids encontrados\n",
    "                contains = True\n",
    "                for i in child_w_app:\n",
    "                    if i not in range(inicio_fim[0], inicio_fim[1]):\n",
    "                        contains = False\n",
    "                \n",
    "\n",
    "                if contains:\n",
    "                    \n",
    "                    child_w_app = [tuple(child_w_app)]\n",
    "                \n",
    "        \n",
    "    else: \n",
    "        \n",
    "        child_w_app = None\n",
    "        \n",
    "    len_parent_w_app = len(parent_w_app) if parent_w_app is not None else None\n",
    "    len_child_w_app = len(child_w_app) if child_w_app is not None else None\n",
    "    \n",
    "    if len_parent_w_app == 1 and len_child_w_app == 1:\n",
    "\n",
    "        \n",
    "        # uso de set para não importar a ordem da tupla\n",
    "        pair = (parent_w_app[0], child_w_app[0])\n",
    "        parent_id, child_id = pair\n",
    "        \n",
    "        if type(parent_id) != tuple:\n",
    "            parent_to_child = False\n",
    "            if annotated_sentence[parent_id][\"head_index\"] == child_id:\n",
    "                parent_to_child = True\n",
    "                \n",
    "        else:\n",
    "            parent_to_child = False\n",
    "            for id in parent_id:\n",
    "                if annotated_sentence[id][\"head_index\"] == child_id:\n",
    "                    parent_to_child = True \n",
    "            \n",
    "        \n",
    "        if type(child_id) != tuple:\n",
    "            child_to_parent = False\n",
    "            if annotated_sentence[child_id][\"head_index\"] == parent_id:\n",
    "                child_to_parent = True\n",
    "        else:\n",
    "            child_to_parent = False\n",
    "            for id in child_id:\n",
    "                if annotated_sentence[id][\"head_index\"] == parent_id:\n",
    "                    child_to_parent = True                 \n",
    "\n",
    "            \n",
    "        return {\"response\": {\"pair\":pair, \"dep_parent_to_child\": parent_to_child, \"dep_child_to_parent\": child_to_parent, \"type\": f\"parent_{len_parent_w_app}_child_{len_child_w_app}\"}}\n",
    "    \n",
    "    # se achou 1 e outro não. Vale também para quando um deles não existe (None)\n",
    "    if (len_parent_w_app == 1 and len_child_w_app == 0) or (len_parent_w_app == 0 and len_child_w_app == 1) or (len_parent_w_app == 1 and len_child_w_app == None) or (len_parent_w_app == None and len_child_w_app == 1): \n",
    "        \n",
    "        if len(parent_w_app) == 0:\n",
    "            \n",
    "            parent_id = None\n",
    "        else: \n",
    "            \n",
    "            parent_id = parent_w_app[0]\n",
    "        if len(child_w_app) == 0:\n",
    "            child_id = None\n",
    "        else:\n",
    "            child_id = child_w_app[0]\n",
    "        \n",
    "        pair = (parent_id, child_id)\n",
    "        parent_to_child = False\n",
    "        child_to_parent = False\n",
    "        return {\"response\": {\"pair\":pair, \"dep_parent_to_child\": parent_to_child, \"dep_child_to_parent\": child_to_parent, \"type\": f\"parent_{len_parent_w_app}_child_{len_child_w_app}\"}}\n",
    "        #return \"miss_one\"\n",
    "        \n",
    "        \n",
    "    \n",
    "    return {\"response\": {\"pair\":(None, None), \"dep_parent_to_child\": None, \"dep_child_to_parent\": None, \"type\": f\"parent_{len_parent_w_app}_child_{len_child_w_app}\"}}\n",
    "def get_dependency_direction(\n",
    "    annotated_sentence,\n",
    "    parent_id, \n",
    "    child_id\n",
    "):\n",
    "    \n",
    "    parent_to_child = False     \n",
    "    if parent_id is not None:\n",
    "        if annotated_sentence[parent_id][\"head_index\"] == child_id:\n",
    "            parent_to_child = True\n",
    "     \n",
    "    child_to_parent = False       \n",
    "    if child_id is not None:\n",
    "        if annotated_sentence[child_id][\"head_index\"] == parent_id:\n",
    "            child_to_parent = True\n",
    "        \n",
    "    return parent_to_child, child_to_parent\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "list_rows = []\n",
    "list_errors_match = [] #(debug)\n",
    "list_n_match = [] # (debug)\n",
    "for dict_an in tqdm(list_dict_an):\n",
    "    \n",
    "    edges = dict_an['edges']\n",
    "    \n",
    "    # create dep pairs\n",
    "    dep_pairs = []\n",
    "    for tk_id, an in dict_an['annotated_sentence'].items():\n",
    "        p = an\n",
    "        p_head_id = an['head_index']\n",
    "        p_head = dict_an['annotated_sentence'].get(p_head_id)\n",
    "        # nao salva os pares que possuem os pos que nao podem ser amr\n",
    "        if p['pos'] in pos_filter or p['pos'] in p_head: continue\n",
    "        \n",
    "        dep_pairs.append((tk_id,p_head_id))\n",
    "    \n",
    "    for edge_id, edge_info in edges.items():\n",
    "        \n",
    "        parent, child = edge_info['nodes']\n",
    "        \n",
    "        value = edge_info['value']\n",
    "        \n",
    "        if \"arg\" not in value.casefold(): continue\n",
    "        \n",
    "        if dict_an['dict_aligments'] is None:\n",
    "            \n",
    "            # acha os ids dos nós\n",
    "            response = find_nodes_ids(\n",
    "            annotated_sentence = dict_an['annotated_sentence'],\n",
    "            edge_amr_info = edge_info)\n",
    "            \n",
    "            pair = response['response']['pair']\n",
    "            \n",
    "            \n",
    "            parent_id, child_id = pair\n",
    "                \n",
    "            \n",
    "            dep_parent_to_child = response['response']['dep_parent_to_child']\n",
    "            dep_child_to_parent = response['response']['dep_child_to_parent']\n",
    "            \n",
    "            \n",
    "            type_response = response['response']['type']\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            parent_id = dict_an['dict_aligments'].get(edge_info['nodes_ids'][0], None)\n",
    "            child_id = dict_an['dict_aligments'].get(edge_info['nodes_ids'][1], None) \n",
    "\n",
    "            \n",
    "            pair = (parent_id, child_id)\n",
    "            \n",
    "            dep_parent_to_child, dep_child_to_parent = get_dependency_direction(annotated_sentence=dict_an['annotated_sentence'], parent_id=parent_id, child_id=child_id)\n",
    "            \n",
    "            type_response = \"match_aligments\"\n",
    "            \n",
    "            print(dep_parent_to_child, dep_child_to_parent)\n",
    "\n",
    "        ann_sent = dict_an['annotated_sentence']\n",
    "        \n",
    "        if not dep_parent_to_child and  not dep_child_to_parent:\n",
    "            #print(\"dep\", dep_parent_to_child, dep_child_to_parent)\n",
    "            dep = \"nao_tem_dep\"\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            if dep_parent_to_child and not dep_child_to_parent:\n",
    "                \n",
    "                dep = ann_sent[child_id]['dep']\n",
    "                \n",
    "            elif dep_child_to_parent and not dep_parent_to_child:\n",
    "                \n",
    "                if type(child_id) != tuple:\n",
    "                \n",
    "                    dep = ann_sent[child_id]['dep']\n",
    "                else:\n",
    "                    dep_list = []\n",
    "                    for id in child_id:\n",
    "                        dep_list.append(ann_sent[id]['dep'])\n",
    "                        \n",
    "                    # remove a dep que indica que o token é continuação de um nome (ex: anotonio eduardo, eduardo tem dep flat:name)\n",
    "                    dep_list = [dep for dep in dep_list if dep != \"flat:name\"]\n",
    "                    \n",
    "                    \n",
    "                    if len(dep_list) == 1:\n",
    "                        dep = dep_list[0]\n",
    "                    else:\n",
    "                        raise Exception(\"erro\")    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            else:\n",
    "                # a palavra tem dependencia com ela mesma\n",
    "                if parent_id == child_id:\n",
    "                    \n",
    "                    # pega a dependencia (nao importa se pega do pai ou do filho)\n",
    "                    dep = ann_sent[child_id]['dep']\n",
    "                    \n",
    "                else:\n",
    "                \n",
    "                    raise Exception(\"Há dependencia de duas mãos\") \n",
    "                \n",
    "                \n",
    "                            \n",
    "        #### lembrar de colocar o parent como o token do parent, nao oq veio do amr\n",
    "        new_row = {\n",
    "            \"corpus_name\": dict_an['corpus_name'],\n",
    "            \"id\": dict_an['id'],\n",
    "            \"parent\": remove_num_text(parent) if parent is not None else None,\n",
    "            \"child\":remove_num_text(child) if child is not None else None,\n",
    "            \"label\": edge_info[\"value\"],\n",
    "            \"dep\": dep\n",
    "        }\n",
    "        \n",
    "        # anotacoes inuteis\n",
    "        useless_cols = [\"head_index\", \"dep\", \"id\"]\n",
    "        \n",
    "        if parent_id is not None:\n",
    "            \n",
    "            if type(parent_id) != tuple:\n",
    "                ann_parent = ann_sent[parent_id]\n",
    "                dict_parent = {f'parent_{key}':value for key, value in ann_parent.items() if key not in useless_cols}\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                for key, value in ann_parent.items():\n",
    "                    \n",
    "                    if key not in useless_cols:\n",
    "                        \n",
    "                        values = [ann_sent[id][key] for id in parent_id]\n",
    "                        \n",
    "                        # checa se todos os elementos são iguais\n",
    "                        if all(i == values[0] for i in values):\n",
    "                            # todos os elementos sao iguais\n",
    "                            final_value = values[0]\n",
    "                        else:\n",
    "                            # sao diferentes\n",
    "                            final_value = \" \".join(values)\n",
    "                            \n",
    "                        \n",
    "                        dict_parent.update({f'parent_{key}': final_value})\n",
    "            \n",
    "        else: \n",
    "            dict_parent = {f'parent_{key}':\"token_nao_encontrado_no_texto\" for key in ['text', 'lemma', 'pos', 'tag', 'shape', 'is_alpha', 'is_stop', 'morph', 'ner']}\n",
    "        \n",
    "        \n",
    "        # cria dict com as features\n",
    "        if child_id is not None:\n",
    "            \n",
    "            if type(child_id) != tuple:\n",
    "                ann_child = ann_sent[child_id]\n",
    "                dict_child = {f'child_{key}':value for key, value in ann_child.items() if key not in useless_cols}\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                for key, value in ann_child.items():\n",
    "                    \n",
    "                    if key not in useless_cols:\n",
    "                        \n",
    "                        values = [ann_sent[id][key] for id in child_id]\n",
    "                        \n",
    "                        # checa se todos os elementos são iguais\n",
    "                        if all(i == values[0] for i in values):\n",
    "                            # todos os elementos sao iguais\n",
    "                            final_value = values[0]\n",
    "                        else:\n",
    "                            # sao diferentes\n",
    "                            final_value = \" \".join([str(value) for value in values])\n",
    "                            \n",
    "                        \n",
    "                        dict_child.update({f'child_{key}': final_value})\n",
    "            \n",
    "        else:\n",
    "            dict_child = {f'child_{key}':\"token_nao_encontrado_no_texto\" for key in ['text', 'lemma', 'pos', 'tag', 'shape', 'is_alpha', 'is_stop', 'morph', 'ner']}\n",
    "\n",
    "\n",
    "        new_row.update(dict_parent)\n",
    "        new_row.update(dict_child)        \n",
    "        list_rows.append(new_row)\n",
    "        list_errors_match.append(type_response)\n",
    "            \n",
    "        \n",
    "df = pd.DataFrame(list_rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def create_alignment(text):\n",
    "    # Extrai os tokens do texto anotado\n",
    "    tokens_line = \"\"\n",
    "    amr_line = \"\"\n",
    "    \n",
    "    for line in text.split('\\n'):\n",
    "        if line.startswith(\"# ::tok\"):\n",
    "            tokens_line = line\n",
    "        elif not line.startswith(\"#\"):\n",
    "            amr_line = line\n",
    "    \n",
    "    if not tokens_line or not amr_line:\n",
    "        return None\n",
    "    \n",
    "    # Remove o prefixo e separa os tokens\n",
    "    tokens = tokens_line.replace(\"# ::tok \", \"\").strip().split()\n",
    "    \n",
    "    # Extrai os nós do AMR usando regex para encontrar padrões (nós)\n",
    "    nodes = re.findall(r'\\((\\S+)', amr_line)\n",
    "    \n",
    "    # Se o número de tokens não coincide com o número de nós, retornamos None\n",
    "    if len(tokens) != len(nodes):\n",
    "        return None\n",
    "    \n",
    "    # Cria o dicionário de alinhamentos\n",
    "    alignment_dict = {node: i for i, node in enumerate(nodes)}\n",
    "    \n",
    "    return alignment_dict\n",
    "\n",
    "# Exemplos de uso\n",
    "text_without_alignments = \"\"\"# ::id poder-2-35-33\n",
    "# ::tok Tem que voltar já .\n",
    "# ::tok-en \n",
    "# ::alignments-bren \n",
    "(o / obligate-01~e.1 :ARG2 (v / voltar-01~e.2 :ARG1 (e / ele~e.0) :time (j / já~e.3)))\"\"\"\n",
    "\n",
    "print(create_alignment(text_without_alignments))  # Saída esperada: {'o': 0, 'v': 1, 'e': 2, 'j': 3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['corpus_name', 'id', 'parent', 'child', 'label', 'dep', 'parent_text',\n",
       "       'parent_lemma', 'parent_pos', 'parent_tag', 'parent_shape',\n",
       "       'parent_is_alpha', 'parent_is_stop', 'parent_morph', 'parent_ner',\n",
       "       'child_text', 'child_lemma', 'child_pos', 'child_tag', 'child_shape',\n",
       "       'child_is_alpha', 'child_is_stop', 'child_morph', 'child_ner',\n",
       "       'child_ner_start_end', 'parent_ner_start_end'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['match_aligments' '1074']\n",
      " ['parent_0_child_0' '1052']\n",
      " ['parent_0_child_1' '1309']\n",
      " ['parent_0_child_2' '77']\n",
      " ['parent_0_child_3' '8']\n",
      " ['parent_0_child_4' '2']\n",
      " ['parent_0_child_None' '1']\n",
      " ['parent_1_child_0' '1085']\n",
      " ['parent_1_child_1' '2212']\n",
      " ['parent_1_child_2' '113']\n",
      " ['parent_1_child_3' '15']\n",
      " ['parent_1_child_4' '5']\n",
      " ['parent_2_child_0' '23']\n",
      " ['parent_2_child_1' '46']\n",
      " ['parent_2_child_2' '10']\n",
      " ['parent_3_child_0' '3']\n",
      " ['parent_3_child_1' '4']\n",
      " ['parent_3_child_2' '5']\n",
      " ['parent_3_child_3' '1']\n",
      " ['parent_4_child_1' '1']]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(list_errors_match, return_counts=True) \n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1096034/2633441891.py:2: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.drop(df.index[df.applymap(lambda x: x == 'token_nao_encontrado_no_texto').any(axis=1)])\n"
     ]
    }
   ],
   "source": [
    "# dropar todas as linhas que tenham token_nao_encontrado_no_texto\n",
    "df = df.drop(df.index[df.applymap(lambda x: x == 'token_nao_encontrado_no_texto').any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3070, 26)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/processed/processed_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
