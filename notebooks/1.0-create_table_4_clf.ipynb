{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import json\n",
    "#import stanza\n",
    "#stanza.download('pt')\n",
    "import numpy as np\n",
    "# install and import amr utils\n",
    "sys.path.append('../')\n",
    "sys.path.append(\"../src/\")\n",
    "from features.nlp_preprocess import *\n",
    "#!pip install ../amr-utils\n",
    "from amr_utils.amr_readers import AMR_Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = '../POS-tagger-portuguese-nltk/trained_POS_taggers/'\n",
    "# tagger_nltk = joblib.load(folder+'POS_tagger_brill.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ao salvar em json ha algumas desformatacoes, vamos consertar \n",
    "def convert_keys_to_int(data):\n",
    "    if 'annotated_sentence' in data.keys():\n",
    "        data['annotated_sentence'] = {int(k): v for k, v in data['annotated_sentence'].items()}\n",
    "    else: print(\"Errro\")\n",
    "    return data\n",
    "\n",
    "# with open('../data/processed/annotated_text.json', 'r') as f:\n",
    "#     list_dict_an = json.loads(f.read())\n",
    "\n",
    "with open('../data/processed/annotated_text.json', 'r') as f:\n",
    "    list_dict_an = json.loads(f.read())\n",
    "    \n",
    "with open('../data/processed/annotated_text_propbank.json', 'r') as f:\n",
    "    list_dict_an_propbank = json.loads(f.read())\n",
    "    \n",
    "    \n",
    "list_dict_an = [convert_keys_to_int(dict) for dict in list_dict_an]\n",
    "list_dict_an_propbank = [convert_keys_to_int(dict) for dict in list_dict_an_propbank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2959\n",
      "3348\n"
     ]
    }
   ],
   "source": [
    "print(len(list_dict_an))\n",
    "print(len(list_dict_an_propbank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1',\n",
       " 'nodes': {'1': 'ter-01',\n",
       "  '1.1': 'person',\n",
       "  '1.1.1': 'name',\n",
       "  '1.1.2': 'explicar-01',\n",
       "  '1.1.2.2': 'coisa',\n",
       "  '1.1.2.2.1': 'esse',\n",
       "  '1.1.2.2.2': 'resultar-01',\n",
       "  '1.1.1.1': '\"Meyer\"',\n",
       "  '1.1.2.3': '2'},\n",
       " 'edges': {'edge 1': {'nodes_ids': ['1.1.1', '1.1.1.1'],\n",
       "   'nodes': ['name', '\"Meyer\"'],\n",
       "   'value': ':op1'},\n",
       "  'edge 2': {'nodes_ids': ['1.1.2', '1.1.2.3'],\n",
       "   'nodes': ['explicar-01', '2'],\n",
       "   'value': ':quant'},\n",
       "  'edge 3': {'nodes_ids': ['1', '1.1'],\n",
       "   'nodes': ['ter-01', 'person'],\n",
       "   'value': ':ARG0'},\n",
       "  'edge 4': {'nodes_ids': ['1.1', '1.1.1'],\n",
       "   'nodes': ['person', 'name'],\n",
       "   'value': ':name'},\n",
       "  'edge 5': {'nodes_ids': ['1.1', '1.1.2'],\n",
       "   'nodes': ['person', 'explicar-01'],\n",
       "   'value': ':ARG1'},\n",
       "  'edge 6': {'nodes_ids': ['1.1.2', '1.1'],\n",
       "   'nodes': ['explicar-01', 'person'],\n",
       "   'value': ':ARG0'},\n",
       "  'edge 7': {'nodes_ids': ['1.1.2', '1.1.2.2'],\n",
       "   'nodes': ['explicar-01', 'coisa'],\n",
       "   'value': ':ARG1'},\n",
       "  'edge 8': {'nodes_ids': ['1.1.2.2', '1.1.2.2.1'],\n",
       "   'nodes': ['coisa', 'esse'],\n",
       "   'value': ':mod'},\n",
       "  'edge 9': {'nodes_ids': ['1.1.2.2', '1.1.2.2.2'],\n",
       "   'nodes': ['coisa', 'resultar-01'],\n",
       "   'value': ':ARG2-of'}},\n",
       " 'snt': 'Meyer tem duas explicações para esses resultados.',\n",
       " 'graph': '(t/ter-01\\n\\t:ARG0 (p/person\\n\\t\\t:ARG1 (e/explicar-01\\n\\t\\t\\t:ARG0 p\\n\\t\\t\\t:ARG1 (c/coisa\\n\\t\\t\\t\\t:ARG2-of (r/resultar-01)\\n\\t\\t\\t\\t:mod (e2/esse))\\n\\t\\t\\t:quant 2)\\n\\t\\t:name (n/name\\n\\t\\t\\t:op1 \"Meyer\")))',\n",
       " 'tok pt': [],\n",
       " 'tokens_nltk': ['Meyer',\n",
       "  'tem',\n",
       "  'duas',\n",
       "  'explicações',\n",
       "  'para',\n",
       "  'esses',\n",
       "  'resultados',\n",
       "  '.'],\n",
       " 'corpus_name': 'sci',\n",
       " 'dict_aligments': None,\n",
       " 'sentence': 'Meyer tem duas explicações para esses resultados.',\n",
       " 'tokens': ['Meyer',\n",
       "  'tem',\n",
       "  'duas',\n",
       "  'explicações',\n",
       "  'para',\n",
       "  'esses',\n",
       "  'resultados',\n",
       "  '.'],\n",
       " 'annotated_sentence': {0: {'text': 'Meyer',\n",
       "   'lemma': 'Meyer',\n",
       "   'pos': 'PROPN',\n",
       "   'tag': 'PROPN',\n",
       "   'dep': 'nsubj',\n",
       "   'shape': 'Xxxxx',\n",
       "   'is_alpha': True,\n",
       "   'is_stop': False,\n",
       "   'morph': 'Gender=Masc|Number=Sing',\n",
       "   'head_index': 1,\n",
       "   'ner': 'PER',\n",
       "   'ner_start_end': [0, 1]},\n",
       "  1: {'text': 'tem',\n",
       "   'lemma': 'ter',\n",
       "   'pos': 'VERB',\n",
       "   'tag': 'VERB',\n",
       "   'dep': 'ROOT',\n",
       "   'shape': 'xxx',\n",
       "   'is_alpha': True,\n",
       "   'is_stop': True,\n",
       "   'morph': 'Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin',\n",
       "   'head_index': 1,\n",
       "   'ner': None},\n",
       "  2: {'text': 'duas',\n",
       "   'lemma': 'dois',\n",
       "   'pos': 'NUM',\n",
       "   'tag': 'NUM',\n",
       "   'dep': 'nummod',\n",
       "   'shape': 'xxxx',\n",
       "   'is_alpha': True,\n",
       "   'is_stop': True,\n",
       "   'morph': 'NumType=Card',\n",
       "   'head_index': 3,\n",
       "   'ner': None},\n",
       "  3: {'text': 'explicações',\n",
       "   'lemma': 'explicação',\n",
       "   'pos': 'NOUN',\n",
       "   'tag': 'NOUN',\n",
       "   'dep': 'obj',\n",
       "   'shape': 'xxxx',\n",
       "   'is_alpha': True,\n",
       "   'is_stop': False,\n",
       "   'morph': 'Gender=Fem|Number=Plur',\n",
       "   'head_index': 1,\n",
       "   'ner': None},\n",
       "  4: {'text': 'para',\n",
       "   'lemma': 'para',\n",
       "   'pos': 'ADP',\n",
       "   'tag': 'ADP',\n",
       "   'dep': 'case',\n",
       "   'shape': 'xxxx',\n",
       "   'is_alpha': True,\n",
       "   'is_stop': True,\n",
       "   'morph': '',\n",
       "   'head_index': 6,\n",
       "   'ner': None},\n",
       "  5: {'text': 'esses',\n",
       "   'lemma': 'esse',\n",
       "   'pos': 'DET',\n",
       "   'tag': 'DET',\n",
       "   'dep': 'det',\n",
       "   'shape': 'xxxx',\n",
       "   'is_alpha': True,\n",
       "   'is_stop': True,\n",
       "   'morph': 'Gender=Masc|Number=Plur|PronType=Dem',\n",
       "   'head_index': 6,\n",
       "   'ner': None},\n",
       "  6: {'text': 'resultados',\n",
       "   'lemma': 'resultado',\n",
       "   'pos': 'NOUN',\n",
       "   'tag': 'NOUN',\n",
       "   'dep': 'nmod',\n",
       "   'shape': 'xxxx',\n",
       "   'is_alpha': True,\n",
       "   'is_stop': False,\n",
       "   'morph': 'Gender=Masc|Number=Plur',\n",
       "   'head_index': 3,\n",
       "   'ner': None},\n",
       "  7: {'text': '.',\n",
       "   'lemma': '.',\n",
       "   'pos': 'PUNCT',\n",
       "   'tag': 'PUNCT',\n",
       "   'dep': 'punct',\n",
       "   'shape': '.',\n",
       "   'is_alpha': False,\n",
       "   'is_stop': False,\n",
       "   'morph': '',\n",
       "   'head_index': 1,\n",
       "   'ner': None}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_dict_an[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict_an_original = list_dict_an.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict_an =  list_dict_an + list_dict_an_propbank\n",
    "#list_dict_an =  list_dict_an_propbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6307"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_dict_an)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos que não podem estar contidas no amr (descritas no artigo)\n",
    "# punct tambem removi pq tbm nao pode e esta no artigo de certa forma\n",
    "pos_filter = [\"SCONJ\", \"DET\", \"ADJ\", \"PUNCT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6307/6307 [00:00<00:00, 9285.11it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus_name</th>\n",
       "      <th>id</th>\n",
       "      <th>parent</th>\n",
       "      <th>child</th>\n",
       "      <th>label</th>\n",
       "      <th>dep</th>\n",
       "      <th>parent_text</th>\n",
       "      <th>parent_lemma</th>\n",
       "      <th>parent_pos</th>\n",
       "      <th>parent_tag</th>\n",
       "      <th>...</th>\n",
       "      <th>child_lemma</th>\n",
       "      <th>child_pos</th>\n",
       "      <th>child_tag</th>\n",
       "      <th>child_shape</th>\n",
       "      <th>child_is_alpha</th>\n",
       "      <th>child_is_stop</th>\n",
       "      <th>child_morph</th>\n",
       "      <th>child_ner</th>\n",
       "      <th>child_ner_start_end</th>\n",
       "      <th>parent_ner_start_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sci</td>\n",
       "      <td>1</td>\n",
       "      <td>ter</td>\n",
       "      <td>person</td>\n",
       "      <td>:ARG0</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>tem</td>\n",
       "      <td>ter</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>...</td>\n",
       "      <td>Meyer</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Gender=Masc|Number=Sing</td>\n",
       "      <td>PER</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sci</td>\n",
       "      <td>1</td>\n",
       "      <td>person</td>\n",
       "      <td>explicar</td>\n",
       "      <td>:ARG1</td>\n",
       "      <td>nao_tem_dep</td>\n",
       "      <td>Meyer</td>\n",
       "      <td>Meyer</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>...</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sci</td>\n",
       "      <td>1</td>\n",
       "      <td>explicar</td>\n",
       "      <td>person</td>\n",
       "      <td>:ARG0</td>\n",
       "      <td>nao_tem_dep</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>...</td>\n",
       "      <td>Meyer</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Gender=Masc|Number=Sing</td>\n",
       "      <td>PER</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sci</td>\n",
       "      <td>1</td>\n",
       "      <td>explicar</td>\n",
       "      <td>coisa</td>\n",
       "      <td>:ARG1</td>\n",
       "      <td>nao_tem_dep</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>...</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sci</td>\n",
       "      <td>1</td>\n",
       "      <td>coisa</td>\n",
       "      <td>resultar</td>\n",
       "      <td>:ARG2-of</td>\n",
       "      <td>nao_tem_dep</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>...</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>token_nao_encontrado_no_texto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17217</th>\n",
       "      <td>PropBankBr_v1.1_Dep</td>\n",
       "      <td>propbank_3347</td>\n",
       "      <td>ser</td>\n",
       "      <td>Abram_Szajman</td>\n",
       "      <td>ARG1</td>\n",
       "      <td>nao_tem_dep</td>\n",
       "      <td>é</td>\n",
       "      <td>ser</td>\n",
       "      <td>AUX</td>\n",
       "      <td>AUX</td>\n",
       "      <td>...</td>\n",
       "      <td>Abram_Szajman</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>Xxxxx_Xxxxx</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Gender=Masc|Number=Sing</td>\n",
       "      <td>LOC</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17218</th>\n",
       "      <td>PropBankBr_v1.1_Dep</td>\n",
       "      <td>propbank_3347</td>\n",
       "      <td>ser</td>\n",
       "      <td>presidente</td>\n",
       "      <td>ARG2</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>é</td>\n",
       "      <td>ser</td>\n",
       "      <td>AUX</td>\n",
       "      <td>AUX</td>\n",
       "      <td>...</td>\n",
       "      <td>presidente</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Gender=Masc|Number=Sing</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17219</th>\n",
       "      <td>PropBankBr_v1.1_Dep</td>\n",
       "      <td>propbank_3348</td>\n",
       "      <td>surpreender</td>\n",
       "      <td>que</td>\n",
       "      <td>ARG0</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>surpreendeu</td>\n",
       "      <td>surpreender</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>...</td>\n",
       "      <td>que</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRON</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Gender=Masc|Number=Sing|PronType=Rel</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17220</th>\n",
       "      <td>PropBankBr_v1.1_Dep</td>\n",
       "      <td>propbank_3348</td>\n",
       "      <td>surpreender</td>\n",
       "      <td>galera</td>\n",
       "      <td>ARG1</td>\n",
       "      <td>obj</td>\n",
       "      <td>surpreendeu</td>\n",
       "      <td>surpreender</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>...</td>\n",
       "      <td>galera</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Gender=Fem|Number=Sing</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17221</th>\n",
       "      <td>PropBankBr_v1.1_Dep</td>\n",
       "      <td>propbank_3348</td>\n",
       "      <td>revelar</td>\n",
       "      <td>côté</td>\n",
       "      <td>ARG1</td>\n",
       "      <td>obj</td>\n",
       "      <td>revelar</td>\n",
       "      <td>revelar</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>...</td>\n",
       "      <td>côté</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Gender=Fem|Number=Sing</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17222 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               corpus_name             id       parent          child  \\\n",
       "0                      sci              1          ter         person   \n",
       "1                      sci              1       person       explicar   \n",
       "2                      sci              1     explicar         person   \n",
       "3                      sci              1     explicar          coisa   \n",
       "4                      sci              1        coisa       resultar   \n",
       "...                    ...            ...          ...            ...   \n",
       "17217  PropBankBr_v1.1_Dep  propbank_3347          ser  Abram_Szajman   \n",
       "17218  PropBankBr_v1.1_Dep  propbank_3347          ser     presidente   \n",
       "17219  PropBankBr_v1.1_Dep  propbank_3348  surpreender            que   \n",
       "17220  PropBankBr_v1.1_Dep  propbank_3348  surpreender         galera   \n",
       "17221  PropBankBr_v1.1_Dep  propbank_3348      revelar           côté   \n",
       "\n",
       "          label          dep                    parent_text  \\\n",
       "0         :ARG0        nsubj                            tem   \n",
       "1         :ARG1  nao_tem_dep                          Meyer   \n",
       "2         :ARG0  nao_tem_dep  token_nao_encontrado_no_texto   \n",
       "3         :ARG1  nao_tem_dep  token_nao_encontrado_no_texto   \n",
       "4      :ARG2-of  nao_tem_dep  token_nao_encontrado_no_texto   \n",
       "...         ...          ...                            ...   \n",
       "17217      ARG1  nao_tem_dep                              é   \n",
       "17218      ARG2         ROOT                              é   \n",
       "17219      ARG0        nsubj                    surpreendeu   \n",
       "17220      ARG1          obj                    surpreendeu   \n",
       "17221      ARG1          obj                        revelar   \n",
       "\n",
       "                        parent_lemma                     parent_pos  \\\n",
       "0                                ter                           VERB   \n",
       "1                              Meyer                          PROPN   \n",
       "2      token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "3      token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "4      token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "...                              ...                            ...   \n",
       "17217                            ser                            AUX   \n",
       "17218                            ser                            AUX   \n",
       "17219                    surpreender                           VERB   \n",
       "17220                    surpreender                           VERB   \n",
       "17221                        revelar                           VERB   \n",
       "\n",
       "                          parent_tag  ...                    child_lemma  \\\n",
       "0                               VERB  ...                          Meyer   \n",
       "1                              PROPN  ...  token_nao_encontrado_no_texto   \n",
       "2      token_nao_encontrado_no_texto  ...                          Meyer   \n",
       "3      token_nao_encontrado_no_texto  ...  token_nao_encontrado_no_texto   \n",
       "4      token_nao_encontrado_no_texto  ...  token_nao_encontrado_no_texto   \n",
       "...                              ...  ...                            ...   \n",
       "17217                            AUX  ...                  Abram_Szajman   \n",
       "17218                            AUX  ...                     presidente   \n",
       "17219                           VERB  ...                            que   \n",
       "17220                           VERB  ...                         galera   \n",
       "17221                           VERB  ...                           côté   \n",
       "\n",
       "                           child_pos                      child_tag  \\\n",
       "0                              PROPN                          PROPN   \n",
       "1      token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "2                              PROPN                          PROPN   \n",
       "3      token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "4      token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "...                              ...                            ...   \n",
       "17217                          PROPN                          PROPN   \n",
       "17218                           NOUN                           NOUN   \n",
       "17219                           PRON                           PRON   \n",
       "17220                           NOUN                           NOUN   \n",
       "17221                           NOUN                           NOUN   \n",
       "\n",
       "                         child_shape                 child_is_alpha  \\\n",
       "0                              Xxxxx                           True   \n",
       "1      token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "2                              Xxxxx                           True   \n",
       "3      token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "4      token_nao_encontrado_no_texto  token_nao_encontrado_no_texto   \n",
       "...                              ...                            ...   \n",
       "17217                    Xxxxx_Xxxxx                          False   \n",
       "17218                           xxxx                           True   \n",
       "17219                            xxx                           True   \n",
       "17220                           xxxx                           True   \n",
       "17221                           xxxx                           True   \n",
       "\n",
       "                       child_is_stop                           child_morph  \\\n",
       "0                              False               Gender=Masc|Number=Sing   \n",
       "1      token_nao_encontrado_no_texto         token_nao_encontrado_no_texto   \n",
       "2                              False               Gender=Masc|Number=Sing   \n",
       "3      token_nao_encontrado_no_texto         token_nao_encontrado_no_texto   \n",
       "4      token_nao_encontrado_no_texto         token_nao_encontrado_no_texto   \n",
       "...                              ...                                   ...   \n",
       "17217                          False               Gender=Masc|Number=Sing   \n",
       "17218                          False               Gender=Masc|Number=Sing   \n",
       "17219                           True  Gender=Masc|Number=Sing|PronType=Rel   \n",
       "17220                          False                Gender=Fem|Number=Sing   \n",
       "17221                          False                Gender=Fem|Number=Sing   \n",
       "\n",
       "                           child_ner child_ner_start_end parent_ner_start_end  \n",
       "0                                PER              [0, 1]                  NaN  \n",
       "1      token_nao_encontrado_no_texto                 NaN               [0, 1]  \n",
       "2                                PER              [0, 1]                  NaN  \n",
       "3      token_nao_encontrado_no_texto                 NaN                  NaN  \n",
       "4      token_nao_encontrado_no_texto                 NaN                  NaN  \n",
       "...                              ...                 ...                  ...  \n",
       "17217                            LOC              [0, 1]                  NaN  \n",
       "17218                           None                 NaN                  NaN  \n",
       "17219                           None                 NaN                  NaN  \n",
       "17220                           None                 NaN                  NaN  \n",
       "17221                           None                 NaN                  NaN  \n",
       "\n",
       "[17222 rows x 26 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_nodes_ids(\n",
    "    annotated_sentence,\n",
    "    edge_amr_info\n",
    "):\n",
    "    \n",
    "    parent, child = edge_amr_info['nodes']\n",
    "    \n",
    "    \n",
    "    parent = remove_num_text(parent)\n",
    "    child = remove_num_text(child) \n",
    "    \n",
    "    \n",
    "\n",
    "    if parent != None:\n",
    "        \n",
    "        # acha a posicao de todos os tokens no texto que sao iguais a parent\n",
    "        parent_w_app = [id for id, an in annotated_sentence.items() if (an[\"lemma\"].casefold() == parent.casefold() or an[\"text\"].casefold() == parent.casefold()) and an[\"pos\"] not in pos_filter]\n",
    "        \n",
    "        # todo nome pessoal vira person na anotação, então precisa achar qual nome tem de pessoa tem no texto\n",
    "        if len(parent_w_app) == 0 and parent == \"person\":\n",
    "            \n",
    "            parent_w_app = [id for id, an in annotated_sentence.items() if an[\"ner\"] == \"PER\"]\n",
    "            \n",
    "            if len(parent_w_app) >1:\n",
    "                \n",
    "                # nomes compostos vao ocupar dois tokens. Para verificar se é um nome composto parta olha o inicio e fim do ner\n",
    "                inicio_fim = annotated_sentence[parent_w_app[0]]['ner_start_end'] # basta pegar o inicio e fim do primeiro token\n",
    "                \n",
    "                # verifica se o inicio e fim bate com os ids encontrados\n",
    "                contains = True\n",
    "                for i in parent_w_app:\n",
    "                    if i not in range(inicio_fim[0], inicio_fim[1]):\n",
    "                        contains = False\n",
    "                \n",
    "\n",
    "                if contains:\n",
    "                    \n",
    "                    parent_w_app = [tuple(parent_w_app)]\n",
    "        \n",
    "    else:\n",
    "\n",
    "        parent_w_app = None\n",
    "\n",
    "    if child != None:\n",
    "        \n",
    "        # acha a posicao de todos os tokens no texto que sao iguais a child\n",
    "        child_w_app = [id for id, an in annotated_sentence.items() if (an[\"lemma\"].casefold() == child.casefold() or  an[\"text\"].casefold() == child.casefold()) and an[\"pos\"] not in pos_filter]\n",
    "\n",
    "        # todo nome pessoal vira person na anotação, então precisa achar qual nome tem de pessoa tem no texto\n",
    "        if len(child_w_app) == 0 and child == \"person\":\n",
    "            \n",
    "            child_w_app = [id for id, an in annotated_sentence.items() if an[\"ner\"] == \"PER\"]\n",
    "            \n",
    "            if len(child_w_app) >1:\n",
    "                \n",
    "                # nomes compostos vao ocupar dois tokens. Para verificar se é um nome composto parta olha o inicio e fim do ner\n",
    "                inicio_fim = annotated_sentence[child_w_app[0]]['ner_start_end'] # basta pegar o inicio e fim do primeiro token\n",
    "                \n",
    "                # verifica se o inicio e fim bate com os ids encontrados\n",
    "                contains = True\n",
    "                for i in child_w_app:\n",
    "                    if i not in range(inicio_fim[0], inicio_fim[1]):\n",
    "                        contains = False\n",
    "                \n",
    "\n",
    "                if contains:\n",
    "                    \n",
    "                    child_w_app = [tuple(child_w_app)]\n",
    "                \n",
    "        \n",
    "    else: \n",
    "        \n",
    "        child_w_app = None\n",
    "        \n",
    "    len_parent_w_app = len(parent_w_app) if parent_w_app is not None else None\n",
    "    len_child_w_app = len(child_w_app) if child_w_app is not None else None\n",
    "    \n",
    "    if len_parent_w_app == 1 and len_child_w_app == 1:\n",
    "\n",
    "        \n",
    "        # uso de set para não importar a ordem da tupla\n",
    "        pair = (parent_w_app[0], child_w_app[0])\n",
    "        parent_id, child_id = pair\n",
    "        \n",
    "        if type(parent_id) != tuple:\n",
    "            parent_to_child = False\n",
    "            if annotated_sentence[parent_id][\"head_index\"] == child_id:\n",
    "                parent_to_child = True\n",
    "                \n",
    "        else:\n",
    "            parent_to_child = False\n",
    "            for id in parent_id:\n",
    "                if annotated_sentence[id][\"head_index\"] == child_id:\n",
    "                    parent_to_child = True \n",
    "            \n",
    "        \n",
    "        if type(child_id) != tuple:\n",
    "            child_to_parent = False\n",
    "            if annotated_sentence[child_id][\"head_index\"] == parent_id:\n",
    "                child_to_parent = True\n",
    "        else:\n",
    "            child_to_parent = False\n",
    "            for id in child_id:\n",
    "                if annotated_sentence[id][\"head_index\"] == parent_id:\n",
    "                    child_to_parent = True                 \n",
    "\n",
    "            \n",
    "        return {\"response\": {\"pair\":pair, \"dep_parent_to_child\": parent_to_child, \"dep_child_to_parent\": child_to_parent, \"type\": f\"parent_{len_parent_w_app}_child_{len_child_w_app}\"}}\n",
    "    \n",
    "    # se achou 1 e outro não. Vale também para quando um deles não existe (None)\n",
    "    if (len_parent_w_app == 1 and len_child_w_app == 0) or (len_parent_w_app == 0 and len_child_w_app == 1) or (len_parent_w_app == 1 and len_child_w_app == None) or (len_parent_w_app == None and len_child_w_app == 1): \n",
    "        \n",
    "        if len(parent_w_app) == 0:\n",
    "            \n",
    "            parent_id = None\n",
    "        else: \n",
    "            \n",
    "            parent_id = parent_w_app[0]\n",
    "        if len(child_w_app) == 0:\n",
    "            child_id = None\n",
    "        else:\n",
    "            child_id = child_w_app[0]\n",
    "        \n",
    "        pair = (parent_id, child_id)\n",
    "        parent_to_child = False\n",
    "        child_to_parent = False\n",
    "        return {\"response\": {\"pair\":pair, \"dep_parent_to_child\": parent_to_child, \"dep_child_to_parent\": child_to_parent, \"type\": f\"parent_{len_parent_w_app}_child_{len_child_w_app}\"}}\n",
    "        #return \"miss_one\"\n",
    "        \n",
    "        \n",
    "    \n",
    "    return {\"response\": {\"pair\":(None, None), \"dep_parent_to_child\": None, \"dep_child_to_parent\": None, \"type\": f\"parent_{len_parent_w_app}_child_{len_child_w_app}\"}}\n",
    "def get_dependency_direction(\n",
    "    annotated_sentence,\n",
    "    parent_id, \n",
    "    child_id\n",
    "):\n",
    "    \n",
    "    parent_to_child = False     \n",
    "    if parent_id is not None:\n",
    "        if annotated_sentence[parent_id][\"head_index\"] == child_id:\n",
    "            parent_to_child = True\n",
    "     \n",
    "    child_to_parent = False       \n",
    "    if child_id is not None:\n",
    "        if annotated_sentence[child_id][\"head_index\"] == parent_id:\n",
    "            child_to_parent = True\n",
    "        \n",
    "    return parent_to_child, child_to_parent\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "list_rows = []\n",
    "list_errors_match = [] #(debug)\n",
    "list_n_match = [] # (debug)\n",
    "for dict_an in tqdm(list_dict_an):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        edges = dict_an['edges']\n",
    "\n",
    "        # create dep pairs\n",
    "        dep_pairs = []\n",
    "        for tk_id, an in dict_an['annotated_sentence'].items():\n",
    "            p = an\n",
    "            p_head_id = an['head_index']\n",
    "            p_head = dict_an['annotated_sentence'].get(p_head_id)\n",
    "            # nao salva os pares que possuem os pos que nao podem ser amr\n",
    "            if p['pos'] in pos_filter or p['pos'] in p_head: continue\n",
    "            \n",
    "            dep_pairs.append((tk_id,p_head_id))\n",
    "\n",
    "        for edge_id, edge_info in edges.items():\n",
    "            \n",
    "            parent, child = edge_info['nodes']\n",
    "            \n",
    "            value = edge_info['value']\n",
    "            \n",
    "            if \"arg\" not in value.casefold(): continue\n",
    "            \n",
    "            if dict_an['dict_aligments'] is None:\n",
    "                \n",
    "                \n",
    "                # acha os ids dos nós\n",
    "                response = find_nodes_ids(\n",
    "                annotated_sentence = dict_an['annotated_sentence'],\n",
    "                edge_amr_info = edge_info)\n",
    "                \n",
    "                pair = response['response']['pair']\n",
    "                \n",
    "                \n",
    "                parent_id, child_id = pair\n",
    "                    \n",
    "                \n",
    "                dep_parent_to_child = response['response']['dep_parent_to_child']\n",
    "                dep_child_to_parent = response['response']['dep_child_to_parent']\n",
    "                \n",
    "                \n",
    "                type_response = response['response']['type']\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                \n",
    "                parent_id = dict_an['dict_aligments'].get(edge_info['nodes_ids'][0], None)\n",
    "                child_id = dict_an['dict_aligments'].get(edge_info['nodes_ids'][1], None) \n",
    "\n",
    "                \n",
    "                pair = (parent_id, child_id)\n",
    "                \n",
    "                dep_parent_to_child, dep_child_to_parent = get_dependency_direction(annotated_sentence=dict_an['annotated_sentence'], parent_id=parent_id, child_id=child_id)\n",
    "                \n",
    "                type_response = \"match_aligments\"\n",
    "\n",
    "            ann_sent = dict_an['annotated_sentence']\n",
    "            \n",
    "            if not dep_parent_to_child and  not dep_child_to_parent:\n",
    "                #print(\"dep\", dep_parent_to_child, dep_child_to_parent)\n",
    "                dep = \"nao_tem_dep\"\n",
    "                \n",
    "            else: \n",
    "                \n",
    "                if dep_parent_to_child and not dep_child_to_parent:\n",
    "                    \n",
    "                    dep = ann_sent[child_id]['dep']\n",
    "                    \n",
    "                elif dep_child_to_parent and not dep_parent_to_child:\n",
    "                    \n",
    "                    if type(child_id) != tuple:\n",
    "                    \n",
    "                        dep = ann_sent[child_id]['dep']\n",
    "                    else:\n",
    "                        dep_list = []\n",
    "                        for id in child_id:\n",
    "                            dep_list.append(ann_sent[id]['dep'])\n",
    "                            \n",
    "                        # remove a dep que indica que o token é continuação de um nome (ex: anotonio eduardo, eduardo tem dep flat:name)\n",
    "                        dep_list = [dep for dep in dep_list if dep != \"flat:name\"]\n",
    "                        \n",
    "                        \n",
    "                        if len(dep_list) == 1:\n",
    "                            dep = dep_list[0]\n",
    "                        else:\n",
    "                            raise Exception(\"erro\")    \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                else:\n",
    "                    # a palavra tem dependencia com ela mesma\n",
    "                    if parent_id == child_id:\n",
    "                        \n",
    "                        # pega a dependencia (nao importa se pega do pai ou do filho)\n",
    "                        dep = ann_sent[child_id]['dep']\n",
    "                        \n",
    "                    else:\n",
    "                    \n",
    "                        raise Exception(\"Há dependencia de duas mãos\") \n",
    "                    \n",
    "                    \n",
    "                                \n",
    "            #### lembrar de colocar o parent como o token do parent, nao oq veio do amr\n",
    "            new_row = {\n",
    "                \"corpus_name\": dict_an['corpus_name'],\n",
    "                \"id\": dict_an['id'],\n",
    "                \"parent\": remove_num_text(parent) if parent is not None else None,\n",
    "                \"child\":remove_num_text(child) if child is not None else None,\n",
    "                \"label\": edge_info[\"value\"],\n",
    "                \"dep\": dep\n",
    "            }\n",
    "            \n",
    "            # anotacoes inuteis\n",
    "            useless_cols = [\"head_index\", \"dep\", \"id\"]\n",
    "            \n",
    "            if parent_id is not None:\n",
    "                \n",
    "                if type(parent_id) != tuple:\n",
    "                    ann_parent = ann_sent[parent_id]\n",
    "                    dict_parent = {f'parent_{key}':value for key, value in ann_parent.items() if key not in useless_cols}\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    for key, value in ann_parent.items():\n",
    "                        \n",
    "                        if key not in useless_cols:\n",
    "                            \n",
    "                            values = [ann_sent[id][key] for id in parent_id]\n",
    "                            \n",
    "                            # checa se todos os elementos são iguais\n",
    "                            if all(i == values[0] for i in values):\n",
    "                                # todos os elementos sao iguais\n",
    "                                final_value = values[0]\n",
    "                            else:\n",
    "                                # sao diferentes\n",
    "                                final_value = \" \".join(values)\n",
    "                                \n",
    "                            \n",
    "                            dict_parent.update({f'parent_{key}': final_value})\n",
    "                \n",
    "            else: \n",
    "                dict_parent = {f'parent_{key}':\"token_nao_encontrado_no_texto\" for key in ['text', 'lemma', 'pos', 'tag', 'shape', 'is_alpha', 'is_stop', 'morph', 'ner']}\n",
    "            \n",
    "            \n",
    "            # cria dict com as features\n",
    "            if child_id is not None:\n",
    "                \n",
    "                if type(child_id) != tuple:\n",
    "                    ann_child = ann_sent[child_id]\n",
    "                    dict_child = {f'child_{key}':value for key, value in ann_child.items() if key not in useless_cols}\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    for key, value in ann_child.items():\n",
    "                        \n",
    "                        if key not in useless_cols:\n",
    "                            \n",
    "                            values = [ann_sent[id][key] for id in child_id]\n",
    "                            \n",
    "                            # checa se todos os elementos são iguais\n",
    "                            if all(i == values[0] for i in values):\n",
    "                                # todos os elementos sao iguais\n",
    "                                final_value = values[0]\n",
    "                            else:\n",
    "                                # sao diferentes\n",
    "                                final_value = \" \".join([str(value) for value in values])\n",
    "                                \n",
    "                            \n",
    "                            dict_child.update({f'child_{key}': final_value})\n",
    "                \n",
    "            else:\n",
    "                dict_child = {f'child_{key}':\"token_nao_encontrado_no_texto\" for key in ['text', 'lemma', 'pos', 'tag', 'shape', 'is_alpha', 'is_stop', 'morph', 'ner']}\n",
    "\n",
    "\n",
    "            new_row.update(dict_parent)\n",
    "            new_row.update(dict_child)        \n",
    "            list_rows.append(new_row)\n",
    "            list_errors_match.append(type_response)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "                  \n",
    "        \n",
    "df = pd.DataFrame(list_rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['match_aligments' '11250']\n",
      " ['parent_0_child_0' '1052']\n",
      " ['parent_0_child_1' '1309']\n",
      " ['parent_0_child_2' '77']\n",
      " ['parent_0_child_3' '8']\n",
      " ['parent_0_child_4' '2']\n",
      " ['parent_0_child_None' '1']\n",
      " ['parent_1_child_0' '1085']\n",
      " ['parent_1_child_1' '2212']\n",
      " ['parent_1_child_2' '113']\n",
      " ['parent_1_child_3' '15']\n",
      " ['parent_1_child_4' '5']\n",
      " ['parent_2_child_0' '23']\n",
      " ['parent_2_child_1' '46']\n",
      " ['parent_2_child_2' '10']\n",
      " ['parent_3_child_0' '3']\n",
      " ['parent_3_child_1' '4']\n",
      " ['parent_3_child_2' '5']\n",
      " ['parent_3_child_3' '1']\n",
      " ['parent_4_child_1' '1']]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(list_errors_match, return_counts=True) \n",
    "list_count = np.asarray((unique, counts)).T\n",
    "print(list_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match_alignments 11250\n",
      "match_1_1 2212\n",
      "others_match 3760\n"
     ]
    }
   ],
   "source": [
    "match_alignments = []\n",
    "match_1_1 = []\n",
    "others_match = []\n",
    "for case in list_count:\n",
    "    \n",
    "    if case[0] == 'match_aligments':\n",
    "        match_alignments.append(int(case[1]))\n",
    "    elif case[0] == 'parent_1_child_1':\n",
    "        match_1_1.append(int(case[1]))\n",
    "    else:\n",
    "        others_match.append(int(case[1]))\n",
    "        \n",
    "print(\"match_alignments\", sum(match_alignments))\n",
    "print(\"match_1_1\", sum(match_1_1))\n",
    "print(\"others_match\", sum(others_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corpus_name\n",
       "PropBankBr_v1.1_Dep    10176\n",
       "lp                      3763\n",
       "news                    1471\n",
       "opisums                 1397\n",
       "sci                      415\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('corpus_name').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1074"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11250 - 10176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_427611/544712622.py:2: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_filtered = df.drop(df.index[df.applymap(lambda x: x == 'token_nao_encontrado_no_texto').any(axis=1)])\n"
     ]
    }
   ],
   "source": [
    "# dropar todas as linhas que tenham token_nao_encontrado_no_texto\n",
    "df_filtered = df.drop(df.index[df.applymap(lambda x: x == 'token_nao_encontrado_no_texto').any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "df_filtered.to_csv(\"../data/processed/processed_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/processed/processed_dataset_unfiltered.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
