{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import json\n",
    "#import stanza\n",
    "#stanza.download('pt')\n",
    "import numpy as np\n",
    "# install and import amr utils\n",
    "sys.path.append('../')\n",
    "sys.path.append(\"../src/\")\n",
    "from features.nlp_preprocess import *\n",
    "#!pip install ../amr-utils\n",
    "from amr_utils.amr_readers import AMR_Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = '../POS-tagger-portuguese-nltk/trained_POS_taggers/'\n",
    "# tagger_nltk = joblib.load(folder+'POS_tagger_brill.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dict annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_lp = '../data/raw/little_prince.txt'\n",
    "path_opisums = '../data/raw/opisums.txt'\n",
    "path_news = '../data/raw/news.txt'\n",
    "path_sci = '../data/raw/science.txt'\n",
    "\n",
    "list_paths = [\n",
    "    ('sci',path_sci),\n",
    "    \n",
    "    ('lp', path_lp),\n",
    "    ('opisums',path_opisums),\n",
    "    ('news',path_news),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cannot deinvert attribute: ('s2', ':op2-of', 'and')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "# Running: sci\n",
      "##################################################\n",
      "[amr] Loading AMRs from file: ../data/raw/science.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:00<00:00, 3987.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anotando texto ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:00<00:00, 175.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################\n",
      "# Running: lp\n",
      "##################################################\n",
      "[amr] Loading AMRs from file: ../data/raw/little_prince.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1525/1525 [00:00<00:00, 4368.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anotando texto ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1525/1525 [00:08<00:00, 183.42it/s]\n",
      "ignoring epigraph data for duplicate triple: ('p', ':instance', 'pai')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################\n",
      "# Running: opisums\n",
      "##################################################\n",
      "[amr] Loading AMRs from file: ../data/raw/opisums.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404/404 [00:00<00:00, 2795.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anotando texto ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404/404 [00:02<00:00, 185.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################\n",
      "# Running: news\n",
      "##################################################\n",
      "[amr] Loading AMRs from file: ../data/raw/news.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 870/870 [00:00<00:00, 6918.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anotando texto ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 870/870 [00:04<00:00, 211.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_features = pd.DataFrame({\n",
    "    'sentence_id': [],\n",
    "    \"corpus_name\": [],\n",
    "    'parent': [],\n",
    "    'child': [],\n",
    "    'parent_pos': [],\n",
    "    'child_pos': [],\n",
    "    'parent_ner': [],\n",
    "    'child_ner': [],\n",
    "    'dependency_role': [],\n",
    "    'parent_position': [],\n",
    "    'child_position': [],\n",
    "    'label':[]\n",
    "})\n",
    "\n",
    "nlp_model_spacy = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "list_dict_an = []\n",
    "for corpus_name, path in list_paths:\n",
    "    \n",
    "    print(f\"\"\"##################################################\n",
    "# Running: {corpus_name}\n",
    "##################################################\"\"\")\n",
    "        \n",
    "    # usa reader de amr para fazer o parsing\n",
    "    reader = AMR_Reader()\n",
    "    amrs = reader.load(path, remove_wiki=True)\n",
    "    \n",
    "    # cria bloco de textos anotados\n",
    "    with open(path, 'r') as file:\n",
    "        str_corpus = file.read()\n",
    "    blocos = str_corpus.split('\\n\\n')\n",
    "    blocos = [bloco for bloco in blocos if bloco != \"\"]\n",
    "        \n",
    "    list_dicts = []\n",
    "    # para cada sentenca, cria dict com os nós e vértices\n",
    "    for i,amr in tqdm(enumerate(amrs), total = len(amrs)):\n",
    "        \n",
    "        texto_anotado = blocos[i]\n",
    "        dict_aligments = parse_alignment(texto_anotado)\n",
    "        \n",
    "        amr_string = amr.amr_string() # obtem string formatada amr        \n",
    "        \n",
    "        dict_annotation = amr_to_dict(amr) \n",
    "        dict_annotation.update({'corpus_name': corpus_name})\n",
    "        dict_annotation.update({'dict_aligments': dict_aligments})\n",
    "        list_dicts.append(dict_annotation)\n",
    "        \n",
    "    print('Anotando texto ...')    \n",
    "    for dict_an in tqdm(list_dicts):\n",
    "        \n",
    "        tokens = dict_an['tokens_nltk']\n",
    "        \n",
    "        if tokens == []:\n",
    "            tokens = dict_an['tok pt']\n",
    "        \n",
    "        # tok_pos = pos_tagger_nltk(\n",
    "        # tokenized_sentence = tokens,\n",
    "        # tagger_nltk = tagger_nltk\n",
    "        # )\n",
    "          \n",
    "        dict_spacy = extract_feat_spacy(\n",
    "            text = dict_an['snt'],\n",
    "            nlp_model = nlp_model_spacy)\n",
    "        \n",
    "                \n",
    "        dict_an.update(dict_spacy)\n",
    "        #dict_an.update({'tok pos': tok_pos})\n",
    "        dict_an.update({\"corpus_name\": corpus_name})\n",
    "        \n",
    "        list_dict_an.append(dict_an)\n",
    "\n",
    "    print()\n",
    "    \n",
    "with open('../data/processed/annotated_text.json', 'w') as f:\n",
    "    json.dump(list_dict_an, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
